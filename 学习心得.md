# ALL approach

# 优化的学习心得

* ~~优化算法~~是为了减少训练误差；<B>深度学习</B>是为了减少泛化误差。为了实现后者，需注意“过拟合”问题
* 优化梯度为0时，停止优化，可能是局部最优解也可能是鞍点
* 在处理鞍点问题时，用Hessian矩阵，根据特征值判断，使用<u>高斯约旦消元法</u>
* 将梯度限制得足够小就行，防止梯度消失的情况
* 用琴生不等式来判断凸函数，性质就是凸函数的局部最小值也是全局最小值
* 拉格朗日函数吧𝐿的鞍点是原始约束优化问题的最优解。
* 学习率既不能太大也不能太小。太大会发散，太小收敛速度慢。
* 非凸函数梯度下降时，大概率求得局部最小值中的一个（较差）
* 正向传播是为了计算损失，反向传播是为了计算梯度
* 每次用当前的值减去当前的函数值✖️在此的梯度值，直到梯度值为0，得到局部最小值

### 梯度相关基础

对于目标函数，对梯度再求道生成的矩阵为黑塞矩阵。对于二次型函数，黑塞矩阵为成熟矩阵，与x无关。

目标函数的梯度的雅可比矩阵就是目标函数的Hessian矩阵。<br>和塞矩阵的特征值，形容在该点附近特征向量的凸凹性，越大凸性越强。<br>正定：极小值；负定：极大值；不定矩阵：不是极值。

## 总结的问题





## 关于偏差，噪声的一些知识点

* 真值：在一定的空间或者时间下被测量所体现的真是的数值，真值是一个数量本身所具有的真实值。
* 噪声公式：![](https://img-blog.csdn.net/20180726210738684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

用来衡量我们拿到数据记得数据质量如何，理想的状态下，我们期望的噪声为0。

* 偏差是期望预测与真实标记的误差(bias)，为了计算方便，一般取方差 的平方。

![](https://img-blog.csdn.net/20180726210738684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

# 生成式对抗网络 GAN

> 生成网络Generator：用来生成图片的网络，接收一个随机的噪声，用噪声来生成图片

> 判别网络Discriminator：用来判别图片是否真实的网络，输入是一张图片img，输出是img为真实图片的概率。如果为1，就代表100%是真实的图片，为0，代表不可能为真实图片

<u>生成网络G的目的是努力生成一个图片来骗过判别网络D，判别网络D的目的是努力鉴别出生成出来的图片是家的</u><br>这两个网络在不断博弈中相互进步，达到理想的状态。D(G(noise))=0.5,(即判别网络也不能)鉴别出生成的图片是假的。



# 常用的一些数学语句

argmax is 到达该值点的x，而max 是到达该值点的函数值



# 重要的损失函数

统计学习中常用的损失函数有以下几种：

(1) 0-1损失函数(0-1 lossfunction):<br><b>L(Y,f(X))={1,0,Y≠f(X)Y=f(X)</b>

 (2) 平方损失函数(quadraticloss function)<br><b> L(Y,f(X))=(Y−f(X))2</b>

 (3) 绝对损失函数(absoluteloss function)<br><b> L(Y,f(X))=|Y−f(X)|</b>

 (4) 对数损失函数(logarithmicloss function) 或对数似然损失函数(log-likelihood loss function)<br><b> L(Y,P(Y|X))=−logP(Y|X)</b>

 损失函数越小，模型就越好。

## 损失函数总结：

<b>损失函数可以很好得反映模型与实际数据差距的工具，理解损失函数能够更好得对后续优化工具（梯度下降等）进行分析与理解。很多时候遇到复杂的问题，其实最难的一关是如何写出损失函数。</b>

# 卷积神经网络

定性分为3层：

输入层，隐藏层，输出层。

重点在于隐藏层，是输入层与输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。

### 卷积神经网络值层级结构

![](https://img-blog.csdn.net/20160702205047459)

上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车。<br>我们要做的工作:<br>

*  最左边是输入层，对数据进行一些预处理，比如去均值，归一化，PCA/白化。CNN只是对训练接做去均值这一步。
* 中间层是隐藏层
  * convolution:卷积层计算，线性乘积，求和；
  * RELU：激励层，使用其中一种的激活函数
  * POOL：池化层，简言之，去区域平局或最大
* 最右边是FC，全连接层

<b>卷积计算是CNN的核心</b>

### CNN之卷积计算层

具体的讲解可以看课程地址: [details地址](https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168247366716800213038617%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168247366716800213038617&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-51812459-null-null.142^v86^insert_down28v1,239^v2^insert_chatgpt&utm_term=卷积神经网络&spm=1018.2226.3001.4187)

是为了将“不标准”的图像：仅仅是做了一些像评议，缩放，旋转，微变形等简单的变换的图像。仍然可以准确快速地识别出来。

相比于全局比对，CNN进行的是局部比对。它拿来比对的这个小块我们称之为Features(特征)。

### 卷积的定义

对<b>图像</b>（不同的数据窗口数据）和<b>滤波矩阵</b>（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做<b>内积</b>（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。

    非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。
![](https://img-blog.csdn.net/20160822134955264)

