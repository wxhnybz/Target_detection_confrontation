## 优化的学习心得

* ~~优化算法~~是为了减少训练误差；<B>深度学习</B>是为了减少泛化误差。为了实现后者，需注意“过拟合”问题
* 优化梯度为0时，停止优化，可能是局部最优解也可能是鞍点
* 在处理鞍点问题时，用Hessian矩阵，根据特征值判断，使用<u>高斯约旦消元法</u>
* 将梯度限制得足够小就行，防止梯度消失的情况
* 用琴生不等式来判断凸函数，性质就是凸函数的局部最小值也是全局最小值
* 拉格朗日函数吧𝐿的鞍点是原始约束优化问题的最优解。
* 学习率既不能太大也不能太小。太大会发散，太小收敛速度慢。
* 非凸函数梯度下降时，大概率求得局部最小值中的一个（较差）
* 正向传播是为了计算损失，反向传播是为了计算梯度
* 每次用当前的值减去当前的函数值✖️在此的梯度值，直到梯度值为0，得到局部最小值

### 梯度相关基础

对于目标函数，对梯度再求道生成的矩阵为黑塞矩阵。对于二次型函数，黑塞矩阵为成熟矩阵，与x无关。

目标函数的梯度的雅可比矩阵就是目标函数的Hessian矩阵。<br>和塞矩阵的特征值，形容在该点附近特征向量的凸凹性，越大凸性越强。<br>正定：极小值；负定：极大值；不定矩阵：不是极值。

## 总结的问题





## 关于偏差，噪声的一些知识点

* 真值：在一定的空间或者时间下被测量所体现的真是的数值，真值是一个数量本身所具有的真实值。
* 噪声公式：![](https://img-blog.csdn.net/20180726210738684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

用来衡量我们拿到数据记得数据质量如何，理想的状态下，我们期望的噪声为0。

* 偏差是期望预测与真实标记的误差(bias)，为了计算方便，一般取方差 的平方。

![](https://img-blog.csdn.net/20180726210738684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



## 生成式对抗网络 GAN

> 生成网络Generator：用来生成图片的网络，接收一个随机的噪声，用噪声来生成图片

> 判别网络Discriminator：用来判别图片是否真实的网络，输入是一张图片img，输出是img为真实图片的概率。如果为1，就代表100%是真实的图片，为0，代表不可能为真实图片

<u>生成网络G的目的是努力生成一个图片来骗过判别网络D，判别网络D的目的是努力鉴别出生成出来的图片是家的</u><br>这两个网络在不断博弈中相互进步，达到理想的状态。D(G(noise))=0.5,(即判别网络也不能)鉴别出生成的图片是假的。



